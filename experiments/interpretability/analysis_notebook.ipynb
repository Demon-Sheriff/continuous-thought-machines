{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mechanistic Interpretability of CTM on Maze Navigation\n",
    "\n",
    "## Research Hypothesis\n",
    "\n",
    "The Continuous Thought Machine (CTM) constructs a **\"Virtual Coordinate System\"** dynamically within the Synchronization Matrix ($S_t$) when solving 2D mazes **without positional embeddings**.\n",
    "\n",
    "Specific clusters of neurons likely fire only when the agent \"imagines\" itself at specific $(x,y)$ coordinates in the maze - similar to **Place Cells** in the hippocampus.\n",
    "\n",
    "---\n",
    "\n",
    "## Notebook Overview\n",
    "\n",
    "1. **Setup**: Load model and data\n",
    "2. **Visualization**: Explore internal states across ticks\n",
    "3. **Place Cell Analysis**: Find neurons that correlate with positions\n",
    "4. **Intervention**: Test causal role of position-encoding neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "# Add project root to path\n",
    "PROJECT_ROOT = Path.cwd().parent.parent\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "# Project imports\n",
    "from models.ctm import ContinuousThoughtMachine\n",
    "from data.custom_datasets import MazeImageFolder\n",
    "\n",
    "# Optional wandb\n",
    "try:\n",
    "    import wandb\n",
    "    WANDB_AVAILABLE = True\n",
    "except ImportError:\n",
    "    WANDB_AVAILABLE = False\n",
    "\n",
    "# Plotting settings\n",
    "sns.set_style('darkgrid')\n",
    "plt.rcParams['figure.figsize'] = [12, 8]\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "# Device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CONFIG = {\n",
    "    # Model\n",
    "    'checkpoint_path': str(PROJECT_ROOT / 'checkpoints/mazes/ctm_mazeslarge_D=2048_T=75_M=25.pt'),\n",
    "    \n",
    "    # Data\n",
    "    'data_root': str(PROJECT_ROOT / 'data/mazes'),\n",
    "    'maze_size': 'medium',  # 'small', 'medium', or 'large'\n",
    "    'num_samples': 50,\n",
    "    'batch_size': 8,\n",
    "    \n",
    "    # Analysis\n",
    "    'num_top_neurons': 20,\n",
    "}\n",
    "\n",
    "print(\"Configuration:\")\n",
    "for k, v in CONFIG.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Model\n",
    "\n",
    "The CTM architecture for maze solving:\n",
    "- **Backbone**: ResNet-based feature extractor (no positional embeddings!)\n",
    "- **Synapse Model**: U-Net style MLP for cross-neuron communication\n",
    "- **NLMs**: Private MLPs per neuron processing activation history\n",
    "- **Output**: Synchronization-based predictions\n",
    "\n",
    "Key insight: `positional_embedding_type = 'none'` confirms no explicit position info!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(checkpoint_path, device):\n",
    "    \"\"\"Load CTM model from checkpoint.\"\"\"\n",
    "    print(f\"Loading checkpoint: {checkpoint_path}\")\n",
    "    \n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        raise FileNotFoundError(\n",
    "            f\"Checkpoint not found: {checkpoint_path}\\n\"\n",
    "            f\"Download from: https://drive.google.com/drive/folders/1vSg8T7FqP-guMDk1LU7_jZaQtXFP9sZg\"\n",
    "        )\n",
    "    \n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
    "    model_args = checkpoint['args']\n",
    "    \n",
    "    # Handle legacy arguments\n",
    "    if not hasattr(model_args, 'backbone_type'):\n",
    "        model_args.backbone_type = f'{model_args.resnet_type}-{model_args.resnet_feature_scales[-1]}'\n",
    "    if not hasattr(model_args, 'neuron_select_type'):\n",
    "        model_args.neuron_select_type = 'first-last'\n",
    "    if not hasattr(model_args, 'n_random_pairing_self'):\n",
    "        model_args.n_random_pairing_self = 0\n",
    "    \n",
    "    # Print key config\n",
    "    print(f\"\\nModel Configuration:\")\n",
    "    print(f\"  d_model (neurons): {model_args.d_model}\")\n",
    "    print(f\"  iterations (ticks): {model_args.iterations}\")\n",
    "    print(f\"  memory_length: {model_args.memory_length}\")\n",
    "    print(f\"  positional_embedding_type: {model_args.positional_embedding_type}\")\n",
    "    \n",
    "    prediction_reshaper = [model_args.out_dims // 5, 5]\n",
    "    \n",
    "    model = ContinuousThoughtMachine(\n",
    "        iterations=model_args.iterations,\n",
    "        d_model=model_args.d_model,\n",
    "        d_input=model_args.d_input,\n",
    "        heads=model_args.heads,\n",
    "        n_synch_out=model_args.n_synch_out,\n",
    "        n_synch_action=model_args.n_synch_action,\n",
    "        synapse_depth=model_args.synapse_depth,\n",
    "        memory_length=model_args.memory_length,\n",
    "        deep_nlms=model_args.deep_memory,\n",
    "        memory_hidden_dims=model_args.memory_hidden_dims,\n",
    "        do_layernorm_nlm=model_args.do_normalisation,\n",
    "        backbone_type=model_args.backbone_type,\n",
    "        positional_embedding_type=model_args.positional_embedding_type,\n",
    "        out_dims=model_args.out_dims,\n",
    "        prediction_reshaper=prediction_reshaper,\n",
    "        dropout=0,\n",
    "        neuron_select_type=model_args.neuron_select_type,\n",
    "        n_random_pairing_self=model_args.n_random_pairing_self,\n",
    "    ).to(device)\n",
    "    \n",
    "    state_dict_key = 'state_dict' if 'state_dict' in checkpoint else 'model_state_dict'\n",
    "    model.load_state_dict(checkpoint[state_dict_key], strict=False)\n",
    "    model.eval()\n",
    "    \n",
    "    return model, model_args\n",
    "\n",
    "try:\n",
    "    model, model_args = load_model(CONFIG['checkpoint_path'], device)\n",
    "    print(f\"\\n✓ Model loaded successfully with {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"\\n✗ {e}\")\n",
    "    model = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Maze Data\n",
    "\n",
    "Maze encoding:\n",
    "- **Red (1,0,0)**: Start position\n",
    "- **Green (0,1,0)**: Goal position\n",
    "- **Black (0,0,0)**: Walls\n",
    "- **White (1,1,1)**: Walkable path\n",
    "\n",
    "Output: Sequence of moves [0=Up, 1=Down, 2=Left, 3=Right, 4=Wait]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_maze_data(data_root, maze_size, num_samples, batch_size):\n",
    "    \"\"\"Load maze dataset.\"\"\"\n",
    "    data_path = f\"{data_root}/{maze_size}/test\"\n",
    "    \n",
    "    if not os.path.exists(data_path):\n",
    "        raise FileNotFoundError(\n",
    "            f\"Maze data not found: {data_path}\\n\"\n",
    "            f\"Download from: https://drive.google.com/file/d/1cBgqhaUUtsrll8-o2VY42hPpyBcfFv86/view\"\n",
    "        )\n",
    "    \n",
    "    dataset = MazeImageFolder(\n",
    "        root=data_path,\n",
    "        which_set='test',\n",
    "        maze_route_length=100,\n",
    "        expand_range=True,\n",
    "        trunc=True if num_samples < 1000 else False\n",
    "    )\n",
    "    \n",
    "    if len(dataset) > num_samples:\n",
    "        dataset = torch.utils.data.Subset(dataset, list(range(num_samples)))\n",
    "    \n",
    "    loader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=False, num_workers=0\n",
    "    )\n",
    "    \n",
    "    return loader, dataset\n",
    "\n",
    "try:\n",
    "    dataloader, dataset = load_maze_data(\n",
    "        CONFIG['data_root'], CONFIG['maze_size'], \n",
    "        CONFIG['num_samples'], CONFIG['batch_size']\n",
    "    )\n",
    "    print(f\"\\n✓ Loaded {len(dataset)} mazes\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"\\n✗ {e}\")\n",
    "    dataloader = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualize Sample Maze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataloader is not None:\n",
    "    # Get a sample maze\n",
    "    inputs, targets = next(iter(dataloader))\n",
    "    \n",
    "    # Convert from [-1, 1] to [0, 1] for display\n",
    "    maze_img = ((inputs[0].numpy() + 1) / 2).transpose(1, 2, 0)\n",
    "    solution = targets[0].numpy()\n",
    "    \n",
    "    # Find actual path length (before padding)\n",
    "    path_length = (solution != 4).sum()\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # Show maze\n",
    "    axes[0].imshow(maze_img)\n",
    "    axes[0].set_title(f'Maze ({CONFIG[\"maze_size\"]})')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Show solution statistics\n",
    "    move_names = ['Up', 'Down', 'Left', 'Right', 'Wait']\n",
    "    move_counts = [(solution == i).sum() for i in range(5)]\n",
    "    \n",
    "    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FFEAA7']\n",
    "    axes[1].bar(move_names, move_counts, color=colors)\n",
    "    axes[1].set_title(f'Solution Moves (Path length: {path_length})')\n",
    "    axes[1].set_ylabel('Count')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nMaze shape: {maze_img.shape}\")\n",
    "    print(f\"Solution: {solution[:path_length].tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Collect Internal States\n",
    "\n",
    "Run the model with `track=True` to capture:\n",
    "- **Pre-activations** ($a_t$): Input to NLMs\n",
    "- **Post-activations** ($z_t$): Output of NLMs - **KEY for our analysis!**\n",
    "- **Synchronization** ($S_t$): Pairwise neuron correlations\n",
    "- **Attention**: Where the model looks in the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_internal_states(model, dataloader, device, max_batches=None):\n",
    "    \"\"\"Collect internal states from model.\"\"\"\n",
    "    all_states = {\n",
    "        'pre_activations': [],   # (T, B, D) per batch\n",
    "        'post_activations': [],  # (T, B, D) per batch  \n",
    "        'synch_out': [],         # (T, B, S) per batch\n",
    "        'attention': [],         # (T, B, H, Hf, Wf) per batch\n",
    "        'predictions': [],       # (B, out_dims, T) per batch\n",
    "        'mazes': [],             # (B, H, W, 3) per batch\n",
    "        'solutions': [],         # (B, route_len) per batch\n",
    "    }\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(tqdm(dataloader, desc=\"Collecting states\")):\n",
    "            if max_batches and batch_idx >= max_batches:\n",
    "                break\n",
    "                \n",
    "            inputs = inputs.to(device)\n",
    "            \n",
    "            # Run with tracking\n",
    "            results = model(inputs, track=True)\n",
    "            predictions, certainties, (synch_out, synch_action), pre_act, post_act, attention = results\n",
    "            \n",
    "            # Store\n",
    "            all_states['pre_activations'].append(pre_act)\n",
    "            all_states['post_activations'].append(post_act)\n",
    "            all_states['synch_out'].append(synch_out)\n",
    "            all_states['attention'].append(attention)\n",
    "            all_states['predictions'].append(predictions.cpu().numpy())\n",
    "            all_states['mazes'].append(((inputs.cpu().numpy() + 1) / 2).transpose(0, 2, 3, 1))\n",
    "            all_states['solutions'].append(targets.numpy())\n",
    "    \n",
    "    return all_states\n",
    "\n",
    "if model is not None and dataloader is not None:\n",
    "    states = collect_internal_states(model, dataloader, device, max_batches=5)\n",
    "    \n",
    "    # Print shapes\n",
    "    print(\"\\nCollected State Shapes:\")\n",
    "    for key in ['pre_activations', 'post_activations', 'synch_out', 'attention']:\n",
    "        if states[key]:\n",
    "            print(f\"  {key}: {states[key][0].shape} (per batch)\")\n",
    "else:\n",
    "    print(\"Model or data not loaded. Skipping state collection.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize Neuron Dynamics Across Ticks\n",
    "\n",
    "How do neuron activations evolve as the model \"thinks\" through the maze?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'states' in dir() and states['post_activations']:\n",
    "    # Get first batch, first sample\n",
    "    post_acts = states['post_activations'][0]  # (T, B, D)\n",
    "    T, B, D = post_acts.shape\n",
    "    \n",
    "    sample_idx = 0\n",
    "    activations = post_acts[:, sample_idx, :]  # (T, D)\n",
    "    \n",
    "    # Find most active neurons\n",
    "    neuron_variance = np.var(activations, axis=0)\n",
    "    top_neurons = np.argsort(neuron_variance)[-10:]\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n",
    "    \n",
    "    # Heatmap of top neurons over time\n",
    "    im = axes[0].imshow(activations[:, top_neurons].T, aspect='auto', cmap='RdBu_r')\n",
    "    axes[0].set_xlabel('Tick (t)')\n",
    "    axes[0].set_ylabel('Neuron Index')\n",
    "    axes[0].set_title('Top 10 Most Variable Neurons Over Time')\n",
    "    axes[0].set_yticks(range(10))\n",
    "    axes[0].set_yticklabels(top_neurons)\n",
    "    plt.colorbar(im, ax=axes[0], label='Activation')\n",
    "    \n",
    "    # Line plot of individual neurons\n",
    "    for i, neuron_idx in enumerate(top_neurons[:5]):\n",
    "        axes[1].plot(activations[:, neuron_idx], label=f'Neuron {neuron_idx}', alpha=0.8)\n",
    "    \n",
    "    axes[1].set_xlabel('Tick (t)')\n",
    "    axes[1].set_ylabel('Activation')\n",
    "    axes[1].set_title('Activation Traces of Top 5 Neurons')\n",
    "    axes[1].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nModel has {D} neurons (d_model)\")\n",
    "    print(f\"Running for {T} internal ticks\")\n",
    "else:\n",
    "    print(\"No states collected yet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Place Cell Analysis\n",
    "\n",
    "Core question: Do specific neurons fire at specific maze positions?\n",
    "\n",
    "We'll compute a **spatial information score** for each neuron:\n",
    "- High score = neuron fires consistently at specific (x,y) locations\n",
    "- Low score = neuron fires uniformly regardless of position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trace_path(maze_img, solution):\n",
    "    \"\"\"Trace the solution path to get (row, col) positions.\"\"\"\n",
    "    # Find start (red pixel)\n",
    "    start_mask = (\n",
    "        (maze_img[:,:,0] > 0.9) & \n",
    "        (maze_img[:,:,1] < 0.1) & \n",
    "        (maze_img[:,:,2] < 0.1)\n",
    "    )\n",
    "    start_coords = np.argwhere(start_mask)\n",
    "    \n",
    "    if len(start_coords) == 0:\n",
    "        return []\n",
    "    \n",
    "    current_pos = list(start_coords[0])\n",
    "    positions = [tuple(current_pos)]\n",
    "    \n",
    "    # Direction mappings\n",
    "    deltas = {\n",
    "        0: (-1, 0),  # Up\n",
    "        1: (1, 0),   # Down\n",
    "        2: (0, -1),  # Left\n",
    "        3: (0, 1),   # Right\n",
    "        4: (0, 0),   # Wait\n",
    "    }\n",
    "    \n",
    "    for move in solution:\n",
    "        if move == 4:  # Wait/Stop\n",
    "            positions.append(tuple(current_pos))\n",
    "        else:\n",
    "            delta = deltas.get(int(move), (0, 0))\n",
    "            current_pos[0] += delta[0]\n",
    "            current_pos[1] += delta[1]\n",
    "            positions.append(tuple(current_pos))\n",
    "    \n",
    "    return positions\n",
    "\n",
    "if 'states' in dir() and states['post_activations']:\n",
    "    # Collect position-activation pairs\n",
    "    position_neuron_activations = defaultdict(lambda: defaultdict(list))\n",
    "    \n",
    "    for batch_idx in range(len(states['post_activations'])):\n",
    "        post_acts = states['post_activations'][batch_idx]  # (T, B, D)\n",
    "        mazes = states['mazes'][batch_idx]\n",
    "        solutions = states['solutions'][batch_idx]\n",
    "        \n",
    "        T, B, D = post_acts.shape\n",
    "        \n",
    "        for sample_idx in range(B):\n",
    "            positions = trace_path(mazes[sample_idx], solutions[sample_idx])\n",
    "            if not positions:\n",
    "                continue\n",
    "            \n",
    "            # Map ticks to positions\n",
    "            for t in range(T):\n",
    "                pos_idx = min(t * len(positions) // T, len(positions) - 1)\n",
    "                pos = positions[pos_idx]\n",
    "                \n",
    "                # Store activation for each neuron at this position\n",
    "                for neuron_idx in range(D):\n",
    "                    position_neuron_activations[pos][neuron_idx].append(\n",
    "                        post_acts[t, sample_idx, neuron_idx]\n",
    "                    )\n",
    "    \n",
    "    print(f\"Collected activations at {len(position_neuron_activations)} unique positions\")\n",
    "else:\n",
    "    print(\"No states collected yet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'position_neuron_activations' in dir() and position_neuron_activations:\n",
    "    # Compute place cell scores for each neuron\n",
    "    D = len(list(position_neuron_activations.values())[0])  # Number of neurons\n",
    "    \n",
    "    neuron_scores = {}\n",
    "    neuron_peak_positions = {}\n",
    "    \n",
    "    for neuron_idx in tqdm(range(D), desc=\"Computing place cell scores\"):\n",
    "        pos_means = {}\n",
    "        pos_vars = {}\n",
    "        \n",
    "        for pos, neuron_acts in position_neuron_activations.items():\n",
    "            if neuron_idx in neuron_acts:\n",
    "                acts = neuron_acts[neuron_idx]\n",
    "                pos_means[pos] = np.mean(acts)\n",
    "                pos_vars[pos] = np.var(acts) if len(acts) > 1 else 0\n",
    "        \n",
    "        if pos_means:\n",
    "            spatial_variance = np.var(list(pos_means.values()))\n",
    "            within_variance = np.mean(list(pos_vars.values())) + 1e-6\n",
    "            \n",
    "            score = spatial_variance / within_variance\n",
    "            peak_pos = max(pos_means.keys(), key=lambda p: pos_means[p])\n",
    "            \n",
    "            neuron_scores[neuron_idx] = score\n",
    "            neuron_peak_positions[neuron_idx] = peak_pos\n",
    "    \n",
    "    # Rank neurons\n",
    "    ranked_neurons = sorted(neuron_scores.keys(), key=lambda n: neuron_scores[n], reverse=True)\n",
    "    \n",
    "    print(f\"\\nTop 10 Place Cell Neurons:\")\n",
    "    for i, n in enumerate(ranked_neurons[:10]):\n",
    "        print(f\"  {i+1}. Neuron {n}: score={neuron_scores[n]:.4f}, peak={neuron_peak_positions[n]}\")\n",
    "else:\n",
    "    print(\"No position data collected yet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualize Place Fields\n",
    "\n",
    "Create heatmaps showing where each top neuron fires strongest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'ranked_neurons' in dir() and ranked_neurons:\n",
    "    # Get maze size\n",
    "    maze_size = 39 if CONFIG['maze_size'] in ['small', 'medium'] else 99\n",
    "    \n",
    "    # Plot place fields for top neurons\n",
    "    top_n = min(12, len(ranked_neurons))\n",
    "    n_cols = 4\n",
    "    n_rows = (top_n + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(4*n_cols, 4*n_rows))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for idx, neuron_idx in enumerate(ranked_neurons[:top_n]):\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        # Create place field\n",
    "        place_field = np.zeros((maze_size, maze_size))\n",
    "        counts = np.zeros((maze_size, maze_size))\n",
    "        \n",
    "        for pos, neuron_acts in position_neuron_activations.items():\n",
    "            if neuron_idx in neuron_acts:\n",
    "                row, col = pos\n",
    "                if 0 <= row < maze_size and 0 <= col < maze_size:\n",
    "                    place_field[row, col] = np.mean(neuron_acts[neuron_idx])\n",
    "                    counts[row, col] = 1\n",
    "        \n",
    "        place_field = np.ma.masked_where(counts == 0, place_field)\n",
    "        \n",
    "        im = ax.imshow(place_field, cmap='hot', aspect='equal')\n",
    "        ax.set_title(f\"Neuron {neuron_idx}\\nScore: {neuron_scores[neuron_idx]:.2f}\")\n",
    "        plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "    \n",
    "    # Hide empty subplots\n",
    "    for idx in range(top_n, len(axes)):\n",
    "        axes[idx].set_visible(False)\n",
    "    \n",
    "    plt.suptitle('Place Fields: Neuron Activation vs. Maze Position', fontsize=14, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No ranking available yet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 9. Synchronization vs Activations: Where is Position Encoded?\n\n**Key Experiment**: Compare linear probe decoding of (x,y) position from:\n- **Z_t**: Raw neuron activations (2048 dims)\n- **S_out**: Synchronization output (2080 dims) - captures neuron *correlations*\n\nThis tests the CTM paper's core claim that the Synchronization Matrix is the key representation."
  },
  {
   "cell_type": "markdown",
   "source": "## 10. Activation Patching: Causal Test of Position Neurons\n\n**Question**: Do the neurons we identified as \"position-encoding\" actually cause behavioral changes?\n\n**Method**: \n1. Identify \"position neurons\" (high activation variance across positions)\n2. Patch their activations mid-inference (at tick T=5)\n3. Compare behavior change to random neuron baseline\n\n**Key insight**: Comparing to random neurons is critical - without this baseline, we can't know if any effect is meaningful.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Visualize probe results\nif 'r2_z' in dir() and 'r2_s' in dir():\n    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n    \n    # Bar chart comparison\n    representations = ['Z_t\\n(Activations)', 'S_out\\n(Synchronization)', 'Combined']\n    r2_scores = [r2_z, r2_s, r2_combined]\n    colors = ['#FF6B6B', '#4ECDC4', '#96CEB4']\n    \n    bars = axes[0].bar(representations, r2_scores, color=colors, edgecolor='black', linewidth=1.5)\n    axes[0].set_ylabel('R² Score', fontsize=12)\n    axes[0].set_title('Position Decoding: Synchronization vs Activations', fontsize=12)\n    axes[0].set_ylim(0, 1)\n    \n    # Add value labels\n    for bar, score in zip(bars, r2_scores):\n        axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, \n                    f'{score:.3f}', ha='center', va='bottom', fontsize=11, fontweight='bold')\n    \n    # Scatter plot: Z_t predictions\n    axes[1].scatter(y_test_z[:, 0], y_pred_z[:, 0], alpha=0.5, label='X', c='blue')\n    axes[1].scatter(y_test_z[:, 1], y_pred_z[:, 1], alpha=0.5, label='Y', c='red')\n    axes[1].plot([0, 60], [0, 60], 'k--', alpha=0.5)\n    axes[1].set_xlabel('True Position')\n    axes[1].set_ylabel('Predicted Position')\n    axes[1].set_title(f'Z_t Predictions (R²={r2_z:.3f})')\n    axes[1].legend()\n    \n    # Scatter plot: S_out predictions\n    axes[2].scatter(y_test_s[:, 0], y_pred_s[:, 0], alpha=0.5, label='X', c='blue')\n    axes[2].scatter(y_test_s[:, 1], y_pred_s[:, 1], alpha=0.5, label='Y', c='red')\n    axes[2].plot([0, 60], [0, 60], 'k--', alpha=0.5)\n    axes[2].set_xlabel('True Position')\n    axes[2].set_ylabel('Predicted Position')\n    axes[2].set_title(f'S_out Predictions (R²={r2_s:.3f})')\n    axes[2].legend()\n    \n    plt.tight_layout()\n    plt.savefig(str(PROJECT_ROOT / 'experiments/interpretability/outputs/sync_vs_activations.png'), dpi=150)\n    plt.show()\n    \n    print(\"\\n\" + \"=\"*50)\n    print(\"KEY FINDING: Position is encoded in CORRELATIONS\")\n    print(\"=\"*50)\n    print(f\"S_out (synchronization) R² = {r2_s:.3f}\")\n    print(f\"Z_t (activations) R² = {r2_z:.3f}\")\n    print(f\"Improvement: {((r2_s - r2_z) / r2_z * 100):.1f}%\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "from sklearn.linear_model import Ridge\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score\n\ndef collect_probe_data(model, dataloader, device, num_samples=500):\n    \"\"\"Collect Z_t and S_out along with position labels for probing.\"\"\"\n    z_t_list = []\n    s_out_list = []\n    positions_list = []\n    \n    model.eval()\n    samples_collected = 0\n    \n    with torch.no_grad():\n        for inputs, targets in tqdm(dataloader, desc=\"Collecting probe data\"):\n            if samples_collected >= num_samples:\n                break\n                \n            inputs = inputs.to(device)\n            B = inputs.shape[0]\n            \n            # Get internal states\n            results = model(inputs, track=True)\n            predictions, certainties, (synch_out, synch_action), pre_act, post_act, attention = results\n            \n            # Z_t: post activations at final tick (T, B, D) -> take last tick\n            z_t = post_act[-1]  # (B, D)\n            \n            # S_out: synch_out at final tick (T, B, S) -> take last tick\n            s_out = synch_out[-1]  # (B, S)\n            \n            # Get positions from mazes\n            mazes_np = ((inputs.cpu().numpy() + 1) / 2).transpose(0, 2, 3, 1)\n            solutions_np = targets.numpy()\n            \n            for i in range(B):\n                if samples_collected >= num_samples:\n                    break\n                    \n                positions = trace_path(mazes_np[i], solutions_np[i])\n                if positions and len(positions) > 0:\n                    # Use middle position as representative\n                    mid_pos = positions[len(positions) // 2]\n                    \n                    z_t_list.append(z_t[i].cpu().numpy())\n                    s_out_list.append(s_out[i].cpu().numpy())\n                    positions_list.append(mid_pos)\n                    samples_collected += 1\n    \n    return np.array(z_t_list), np.array(s_out_list), np.array(positions_list)\n\ndef train_position_probe(X, y, name=\"\"):\n    \"\"\"Train Ridge regression probe and return R² score.\"\"\"\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    \n    probe = Ridge(alpha=1.0)\n    probe.fit(X_train, y_train)\n    \n    y_pred = probe.predict(X_test)\n    r2 = r2_score(y_test, y_pred)\n    \n    print(f\"{name} R² = {r2:.4f}\")\n    return r2, probe, y_test, y_pred\n\n# Run the sync matrix vs activations comparison\nif model is not None and dataloader is not None:\n    print(\"Collecting data for probing experiment...\")\n    z_t_data, s_out_data, positions = collect_probe_data(model, dataloader, device, num_samples=200)\n    \n    print(f\"\\nCollected {len(positions)} samples\")\n    print(f\"Z_t shape: {z_t_data.shape}\")\n    print(f\"S_out shape: {s_out_data.shape}\")\n    print(f\"Positions shape: {positions.shape}\")\n    \n    # Train probes\n    print(\"\\n\" + \"=\"*50)\n    print(\"Position Decoding Results:\")\n    print(\"=\"*50)\n    \n    r2_z, probe_z, y_test_z, y_pred_z = train_position_probe(z_t_data, positions, \"Z_t (activations)\")\n    r2_s, probe_s, y_test_s, y_pred_s = train_position_probe(s_out_data, positions, \"S_out (synchronization)\")\n    \n    # Combined\n    combined = np.concatenate([z_t_data, s_out_data], axis=1)\n    r2_combined, _, _, _ = train_position_probe(combined, positions, \"Combined\")\n    \n    print(f\"\\nImprovement: S_out is {((r2_s - r2_z) / r2_z * 100):.1f}% better than Z_t\")\nelse:\n    print(\"Model or data not loaded.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results if analysis was completed\n",
    "if 'ranked_neurons' in dir() and ranked_neurons:\n",
    "    output_dir = str(PROJECT_ROOT / 'experiments/interpretability/outputs')\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    np.savez(\n",
    "        f\"{output_dir}/notebook_results.npz\",\n",
    "        ranked_neurons=ranked_neurons,\n",
    "        neuron_scores=dict(neuron_scores),\n",
    "        neuron_peak_positions=dict(neuron_peak_positions)\n",
    "    )\n",
    "    \n",
    "    print(f\"Results saved to: {output_dir}/notebook_results.npz\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}